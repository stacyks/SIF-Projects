{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5969f13d",
   "metadata": {},
   "source": [
    "# SIF Statistics Homework 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07e10163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8192a",
   "metadata": {},
   "source": [
    "## Lecture Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c288d",
   "metadata": {},
   "source": [
    "Here's a brief review of the mathematics covered in lecture. Let $X$ be a random variable that can take on values $x_1$, $x_2$, ... $x_n$ with probabilities $p(x_1)$, $p(x_2)$, ... $p(x_n)$. $p(x)$ is the probability mass function of $X$. The sum of all probabilities must be 1; that is,\n",
    "\n",
    "$$\\sum_{i=1}^n p(x_i) = 1$$\n",
    "\n",
    "The average value of $X$ is given by \n",
    "\n",
    "$$\\mu = \\mathrm{E}[X] = \\sum_{i=1}^n x_i p(x_i)$$\n",
    "\n",
    "The variance of $X$, which describes how \"spread out\" $X$ is, is given by\n",
    "\n",
    "$$\\sigma^2 = \\mathrm{Var}[X] = \\mathrm{E}[(X - \\mu)^2] = \\sum_{i=1}^n (x_i - \\mu)^2 p(x_i)$$\n",
    "\n",
    "We have the following nice linearity property of expectations: let $X$ and $Y$ be random variables (that may or may not be independent), and let $a$, $b$, and $c$ be constants. Then\n",
    "\n",
    "$$\\mathrm{E}[aX + bY + c] = a\\mathrm{E}[X] + b\\mathrm{E}[Y] + c$$\n",
    "\n",
    "Unfortunately, we do not have a similar property for the product of random variables. That is, $\\mathrm{E}[X \\cdot Y] \\neq \\mathrm{E}[X] \\cdot \\mathrm{E}[Y]$ in the general case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404a5b2",
   "metadata": {},
   "source": [
    "## Problem 1: Variance/Covariance Formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718713b",
   "metadata": {},
   "source": [
    "1. Let $X$ be a random variable, and let $a$ be a constant. Prove that $\\mathrm{Var}[X + a] = \\mathrm{Var}[X]$\n",
    "\n",
    "2. Let $X$ have expected value $\\mu = \\mathrm{E}[X]$ (a constant). Prove the shortcut formula for variance: $\\mathrm{Var}[X] = \\mathrm{E}[X^2] - \\mu^2$\n",
    "\n",
    "Let random variables $X$ and $Y$ have means $\\mu_X$ and $\\mu_Y$ respectively (both known constants). The covariance between $X$ and $Y$ is defined by $$\\mathrm{Cov}[X, Y] = \\mathrm{E}[(X - \\mu_x)(Y - \\mu_Y)]$$\n",
    "\n",
    "3. Prove that covariance scales linearly with each of its arguments; that is, if $a$ and $b$ are constants, then $\\mathrm{Cov}(aX, bY) = ab\\mathrm{Cov}[X, Y]$\n",
    "\n",
    "4. Prove the shortcut formula for covariance: $\\mathrm{Cov}(X, Y) = \\mathrm{E}[XY] - \\mu_X \\mu_Y$\n",
    "\n",
    "5. Starting from the definition of variance, prove the following formula: $$\\mathrm{Var}[X + Y] = \\mathrm{Var}[X] + \\mathrm{Var}[Y] + 2\\mathrm{Cov}[X, Y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ea1a0",
   "metadata": {},
   "source": [
    "## Problem 2: Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9145a",
   "metadata": {},
   "source": [
    "The normal distribution is central to statistical theory due to the Central Limit Theorem. If $X$ has standard normal distribution $N(0, 1)$ (with mean 0 and variane 1), then the cumulative distribution function is given by\n",
    "\n",
    "$$P(X \\leq x) = \\Phi(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\:\\mathrm{d}x$$\n",
    "\n",
    "All normal distributions are simple transformations of the standard normal distribution. For a random variable $Y$ that follows distribution $N(\\mu, \\sigma^2)$ (normal distribution with mean $\\mu$ and variance $\\sigma^2$), then $\\frac{Y - \\mu}{\\sigma}$ follows a standard normal distribution. (This transformation is called normalizing.)\n",
    "\n",
    "1. Generate 1000 samples from a normal distribution using `np.random.normal` and plot them on a histogram (you should get a bell curve).\n",
    "\n",
    "2. Overlay the plot from (1) with the normal distribution function using `scipy.stats.norm` (they should be very similar).\n",
    "\n",
    "The Central Limit Theorem states that, given a sample dataset from any distribution, the mean of the dataset will approximately follow a normal distribution with the same mean as the true distribution, and a variance that shrinks with the number of samples.\n",
    "\n",
    "3. Pick a distribution from `np.random` (such as `np.random.uniform`). Use documentation/Google to find the true mean $\\mu$ and variance $\\sigma$ of this distribution. Perform the following procedure $M = 100$ times: simulate $N = 1000$ samples of this random variable, and find the mean of these samples. This should result in a dataset of $M$ mean values. (a) Find the mean of this dataset. How close is it to the true mean? (b) Normalize the dataset and plot it on a histogram. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5951c2",
   "metadata": {},
   "source": [
    "## Problem 3: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf778523",
   "metadata": {},
   "source": [
    "Linear regression is a simple environment to practice basic modeling and hypothesis testing. Consider the $X$ and $Y$ data generated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9092d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(1, 20, 100)\n",
    "Y = 5*np.sqrt(X) + 3*np.log(21-X) + np.random.normal(0, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbb0d6",
   "metadata": {},
   "source": [
    "1. Use `scipy.stats.linregress` to run a linear regression, and plot it along with the actual data.\n",
    "\n",
    "2. Interpret the $p$-value and $R^2$ value for the regression (use documentation).\n",
    "\n",
    "Simple linear regression coefficients are computed using the least-squares method. Suppose we construct a model $Y = a + bX + \\epsilon$, where $a$ and $b$ are regression constants, and $\\epsilon$ is normally-distributed error. Say we have data $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$. Then the linear regression coefficients are chosen to minimize\n",
    "\n",
    "$$\\min_{a, b} \\sum_{i=1}^n (y_i - (a + bx_i))^2$$\n",
    "\n",
    "3. Use the above formula and `scipy.optimize.minimize` (see documentation) to compute the least-squares estimates (you should obtain the same results as before).\n",
    "\n",
    "4. (Bonus) Derive mathematical formulae for the least-squares estimates using calculus (take partial derivatives and solve the resulting equations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e52750",
   "metadata": {},
   "source": [
    "## Problem 4: Multiple Linear Regression\n",
    "\n",
    "Several interest problems arise with regressions on multiple variables. These methods can be extended to time-series regressions, which we use extensively. Consider the predictors $A$, $B$, $C$, and the true output $Y$ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bcc72745",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.uniform(-2, 10, 100)\n",
    "B = np.random.normal(0, 5, 100)\n",
    "C = 0.3*A + 0.7*B + np.random.normal(0, 0.1, 100)\n",
    "Y = 10*(A + C) + 0.2*((C - A)**2) + np.exp(B/5) + np.random.normal(0, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87154b3",
   "metadata": {},
   "source": [
    "1. Use `sm.OLS` from `statsmodels` to run a multiple linear regression for $Y$ against $A$, $B$, and $C$. Write the mathematical form the the model.\n",
    "\n",
    "2. Based on the $p$-values, which coefficients are likely nonzero?\n",
    "\n",
    "3. Is the model overall a good model? Why or why not?\n",
    "\n",
    "4. Run a regression of $Y$ against 6 variables: the three predictors from before as well as three datasets of independent normal variables (generated using `np.random.normal(0, 1, 100)`). When you run the regression, what are your coefficients and $p$-values for the noise regressors? Is this a problem? If you didn't know this was white-noise, how could you know that these regressors were meaningless?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a34a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
